<div>Teachable Machine Audio Model</div>
<button type="button" onclick="init()">Start</button>
<br /><br />
<span id="hablado"></span>
<br /><br />
<span id="estimated-pitch"></span>
<br /><br />
<div id="label-container"></div>

<script src="/src/ext/tf.min.js"></script>
<script src="/src/ext/speech-commands.min.js"></script>

<script type="text/javascript">
  // more documentation available at
  // https://github.com/tensorflow/tfjs-models/tree/master/speech-commands

  const VOLUME_THRESHOLD = 0.5; // TODO
  const PROB_THRESHOLD = 0.75;
  const OVERLAP_FACTOR = 0.9; // probably want between 0.5 and 0.75, amount of overlap between samples
  //
  const PITCH_CONFIDENCE_THRESHOLD = 0.25; // pitch estimation above this threshold only
  //
  const MINVOLUME = 80, // config para intensity estimation
    MAXVOLUME = 17.5;

  const intclamp = (val) =>
    (Math.min(Math.max(val, -MINVOLUME), -MAXVOLUME) + MINVOLUME) /
    (MINVOLUME - MAXVOLUME);

  // the link to  model provided by Teachable Machine export panel
  const URL = "http://127.0.0.1:8000/models/";
  //"https://teachablemachine.withgoogle.com/models/T-0mi0cUd/"; // 2 voces 100 Epochs

  async function createModel() {
    const checkpointURL = URL + "model.json"; // model topology
    const metadataURL = URL + "metadata.json"; // model metadata

    console.log("Starting engine");

    const recognizer = speechCommands.create(
      "BROWSER_FFT", // fourier transform type, not useful to change
      undefined, // speech commands vocabulary feature, not useful for your models
      checkpointURL,
      metadataURL
    );

    // check that model and metadata are loaded via HTTPS requests.
    await recognizer.ensureModelLoaded();

    return recognizer;
  }

  async function init() {
    const recognizer = await createModel();
    const classLabels = recognizer.wordLabels(); // get class labels
    const labelContainer = document.getElementById("label-container");
    const habladoSpan = document.getElementById("hablado");
    crepe(); // pitch estimator

    for (let i = 0; i < classLabels.length; i++) {
      labelContainer.appendChild(document.createElement("div"));
    }

    // listen() takes two arguments:
    // 1. A callback function that is invoked anytime a word is recognized.
    // 2. A configuration object with adjustable fields
    recognizer.listen(
      (result) => {
        const scores = result.scores; // probability of prediction for each class
        // render the probability scores per class
        let maxLabel,
          maxProb = 0;

        for (let i = 0; i < classLabels.length; i++) {
          const classPrediction =
            classLabels[i] + ": " + result.scores[i].toFixed(2);
          labelContainer.childNodes[i].innerHTML = classPrediction;
          if (result.scores[i] > maxProb) {
            maxProb = result.scores[i];
            maxLabel = classLabels[i];
          }
        }
        if (maxProb > PROB_THRESHOLD) {
          if (habladoSpan.innerText.length > 30) {
            // imprimimos los ultimos fonemas predichos
            habladoSpan.innerText = habladoSpan.innerText.slice(1, 30);
          }
          habladoSpan.innerText = habladoSpan.innerText + maxLabel;
        }

        // Compute intensity
        const amplitudes = result.spectrogram.data.map(Math.exp);
        let total_intensity = amplitudes.reduce((accumulator, currentValue) => {
          return accumulator + currentValue;
        }, 0);
        total_intensity =
          total_intensity > 0 ? Math.log(total_intensity) : -MINVOLUME;
        let loudness = intclamp(total_intensity);
        let tenseness = Math.pow(loudness, 4);
        let voiceness = (Math.acos(1 - tenseness) * 2) / Math.PI;
        console.log(loudness, tenseness, voiceness);
      },
      {
        includeSpectrogram: true, // in case listen should return result.spectrogram
        probabilityThreshold: PROB_THRESHOLD,
        invokeCallbackOnNoiseAndUnknown: false,
        overlapFactor: OVERLAP_FACTOR, // probably want between 0.5 and 0.75. More info in README
      }
    );
  }

  crepe = function () {
    var audioContext;
    var running = false;
    const AudioContext = window.AudioContext || window.webkitAudioContext;
    audioContext = new AudioContext();

    // perform resampling the audio to 16000 Hz, on which the model is trained.
    // setting a sample rate in AudioContext is not supported by most browsers at the moment.
    function resample(audioBuffer, onComplete) {
      const interpolate = audioBuffer.sampleRate % 16000 != 0;
      const multiplier = audioBuffer.sampleRate / 16000;
      const original = audioBuffer.getChannelData(0);
      const subsamples = new Float32Array(1024);
      for (var i = 0; i < 1024; i++) {
        if (!interpolate) {
          subsamples[i] = original[i * multiplier];
        } else {
          // simplistic, linear resampling
          var left = Math.floor(i * multiplier);
          var right = left + 1;
          var p = i * multiplier - left;
          subsamples[i] = (1 - p) * original[left] + p * original[right];
        }
      }
      onComplete(subsamples);
    }

    // bin number -> cent value mapping
    const cent_mapping = tf.add(
      tf.linspace(0, 7180, 360),
      tf.tensor(1997.3794084376191)
    );

    function process_microphone_buffer(event) {
      resample(event.inputBuffer, function (resampled) {
        tf.tidy(() => {
          running = true;

          // run the prediction on the model
          const frame = tf.tensor(resampled.slice(0, 1024));
          const zeromean = tf.sub(frame, tf.mean(frame));
          const framestd = tf.tensor(
            tf.norm(zeromean).dataSync() / Math.sqrt(1024)
          );
          const normalized = tf.div(zeromean, framestd);
          const input = normalized.reshape([1, 1024]);
          const activation = model.predict([input]).reshape([360]);

          // the confidence of voicing activity and the argmax bin
          const confidence = activation.max().dataSync()[0];
          const center = activation.argMax().dataSync()[0];
          console.log("voicing-confidence", confidence.toFixed(3));

          // slice the local neighborhood around the argmax bin
          const start = Math.max(0, center - 4);
          const end = Math.min(360, center + 5);
          const weights = activation.slice([start], [end - start]);
          const cents = cent_mapping.slice([start], [end - start]);

          // take the local weighted average to get the predicted pitch
          const products = tf.mul(weights, cents);
          const productSum = products.dataSync().reduce((a, b) => a + b, 0);
          const weightSum = weights.dataSync().reduce((a, b) => a + b, 0);
          const predicted_cent = productSum / weightSum;
          const predicted_hz = 10 * Math.pow(2, predicted_cent / 1200.0);

          // update the UI and the activation plot
          var result =
            confidence > PITCH_CONFIDENCE_THRESHOLD
              ? predicted_hz.toFixed(3) + " Hz"
              : "&nbsp;no voice&nbsp&nbsp;";
          var strlen = result.length;
          for (var i = 0; i < 11 - strlen; i++) result = "&nbsp;" + result;
          document.getElementById("estimated-pitch").innerHTML = result;
        });
      });
    }

    function initAudio() {
      if (!navigator.getUserMedia) {
        if (navigator.mediaDevices) {
          navigator.getUserMedia = navigator.mediaDevices.getUserMedia;
        } else {
          navigator.getUserMedia =
            navigator.webkitGetUserMedia ||
            navigator.mozGetUserMedia ||
            navigator.msGetUserMedia;
        }
      }
      if (navigator.getUserMedia) {
        navigator.getUserMedia(
          { audio: true },
          function (stream) {
            console.log(
              "Audio context sample rate = " + audioContext.sampleRate
            );
            const mic = audioContext.createMediaStreamSource(stream);

            // We need the buffer size that is a power of two and is longer than 1024 samples when resampled to 16000 Hz.
            // In most platforms where the sample rate is 44.1 kHz or 48 kHz, this will be 4096, giving 10-12 updates/sec.
            const minBufferSize = (audioContext.sampleRate / 16000) * 1024;
            for (
              var bufferSize = 4;
              bufferSize < minBufferSize;
              bufferSize *= 2
            );
            console.log("Buffer size = " + bufferSize);
            const scriptNode = audioContext.createScriptProcessor(
              bufferSize,
              1,
              1
            );
            scriptNode.onaudioprocess = process_microphone_buffer;

            // It seems necessary to connect the stream to a sink for the pipeline to work, contrary to documentataions.
            // As a workaround, here we create a gain node with zero gain, and connect temp to the system audio output.
            const gain = audioContext.createGain();
            gain.gain.setValueAtTime(0, audioContext.currentTime);

            mic.connect(scriptNode);
            scriptNode.connect(gain);
            gain.connect(audioContext.destination);
          },
          function (message) {
            console.log("Could not access microphone - " + message);
          }
        );
      } else
        console.log("Could not access microphone - getUserMedia not available");
    }

    async function initTF() {
      window.model = await tf.loadLayersModel("models/crepe/crepe.json");
      initAudio();
    }

    initTF();

    return {
      audioContext: audioContext,
      resume: function () {
        audioContext.resume();
      },
    };
  };
</script>
